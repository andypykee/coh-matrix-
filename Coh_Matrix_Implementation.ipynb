{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import necessary libraries and download NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e802xK6mVfjC",
        "outputId": "26deb6fe-24d6-45b4-ccc1-b2f39e7c75b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q spacy textstat wordfreq emoji sentence-transformers\n"
      ],
      "metadata": {
        "id": "yqdcJ5LYzQjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DkxtyZLhzQmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "import spacy\n",
        "import textstat\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from wordfreq import word_frequency\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk"
      ],
      "metadata": {
        "id": "Tdgj95dAzQot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"stopwords\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1XX-mm40RIo",
        "outputId": "51eba164-7ef4-4fe0-b39f-ea63bfc1723c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkt09aDp0bRw",
        "outputId": "0339014d-1f5a-47bb-8978-cd0a3fbc369d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# file upload\n",
        "df = pd.read_csv(\"sm_data.csv\")\n",
        "\n",
        "# Basic feature functions\n",
        "def basic_counts(text):\n",
        "    words = word_tokenize(text)\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    return len(words), len(sentences), np.mean([len(s.split()) for s in sentences if s.strip()] or [0])\n",
        "\n",
        "def lexical_diversity(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    unique_words = set(words)\n",
        "    content_words = [w for w in words if w.isalpha() and w not in stopwords.words('english')]\n",
        "    return len(unique_words) / len(words) if words else 0, len(content_words) / len(words) if words else 0\n",
        "\n",
        "def readability_scores(text):\n",
        "    return textstat.flesch_reading_ease(text), textstat.gunning_fog(text), textstat.smog_index(text), textstat.dale_chall_readability_score(text)\n",
        "\n",
        "def syntactic_complexity(text):\n",
        "    doc = nlp(text)\n",
        "    tree_depths = [token.head.i - token.i for token in doc if token.dep_ != 'punct']\n",
        "    noun_phrases = list(doc.noun_chunks)\n",
        "    return len(noun_phrases), np.mean(tree_depths) if tree_depths else 0\n",
        "\n",
        "def word_info(text):\n",
        "    words = word_tokenize(text)\n",
        "    syllables = [textstat.syllable_count(w) for w in words]\n",
        "    freqs = [word_frequency(w, 'en') for w in words if w.isalpha()]\n",
        "    return np.mean(syllables) if syllables else 0, np.mean(freqs) if freqs else 0\n",
        "\n",
        "def tweet_specific(text):\n",
        "    hashtags = len(re.findall(r\"#\\w+\", text))\n",
        "    mentions = len(re.findall(r\"@\\w+\", text))\n",
        "    emojis = emoji.emoji_count(text)\n",
        "    return hashtags, mentions, emojis\n",
        "\n",
        "def psycholinguistic_scores(text):\n",
        "    # Proxy scores using concreteness (word length, familiarity, etc.)\n",
        "    words = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
        "    avg_len = np.mean([len(w) for w in words]) if words else 0\n",
        "    return avg_len\n",
        "\n",
        "def semantic_similarity(text):\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    if len(sentences) < 2:\n",
        "        return 0\n",
        "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
        "    sim = util.pytorch_cos_sim(embeddings[:-1], embeddings[1:]).mean().item()\n",
        "    return sim\n",
        "\n",
        "# Extraction loop\n",
        "features = []\n",
        "for text in df['text']:\n",
        "    wc, sc, asl = basic_counts(text)\n",
        "    ttr, cwr = lexical_diversity(text)\n",
        "    fre, gfi, smog, dale = readability_scores(text)\n",
        "    np_count, tree_depth = syntactic_complexity(text)\n",
        "    avg_syll, avg_freq = word_info(text)\n",
        "    ht_count, mention_count, emoji_count = tweet_specific(text)\n",
        "    avg_word_len = psycholinguistic_scores(text)\n",
        "    semantic_sim = semantic_similarity(text)\n",
        "\n",
        "    features.append({\n",
        "        'word_count': wc,\n",
        "        'sentence_count': sc,\n",
        "        'avg_sentence_length': asl,\n",
        "        'type_token_ratio': ttr,\n",
        "        'content_word_ratio': cwr,\n",
        "        'flesch_reading_ease': fre,\n",
        "        'gunning_fog_index': gfi,\n",
        "        'smog_index': smog,\n",
        "        'dale_chall_score': dale,\n",
        "        'noun_phrase_count': np_count,\n",
        "        'avg_tree_depth': tree_depth,\n",
        "        'avg_syllables_per_word': avg_syll,\n",
        "        'avg_word_frequency': avg_freq,\n",
        "        'hashtag_count': ht_count,\n",
        "        'mention_count': mention_count,\n",
        "        'emoji_count': emoji_count,\n",
        "        'avg_word_length': avg_word_len,\n",
        "        'semantic_similarity': semantic_sim\n",
        "    })\n",
        "\n",
        "# Combine with original\n",
        "features_df = pd.DataFrame(features)\n",
        "result_df = pd.concat([df, features_df], axis=1)\n",
        "\n",
        "# Save output\n",
        "result_df.to_csv(\"sm_data_with_features.csv\", index=False)\n",
        "print(\"Feature extraction complete! Saved to 'sm_data_with_features.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGt9PL9SzJx8",
        "outputId": "ca652ed0-1431-400d-ed42-62e16f1fa657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Feature extraction complete! Saved to 'sm_data_with_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zdAQIBabD-Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84P6RlC9EBg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COH-MATRIX ANALYSIS**"
      ],
      "metadata": {
        "id": "9fAxqfJLAcLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Discriptives Features**"
      ],
      "metadata": {
        "id": "RhfSvNG2EIR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#!pip install spacy\n",
        "#!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "qYiYBnGTzJzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "# Load model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Data Loading\n",
        "df = pd.read_csv(\"sm_data.csv\")\n",
        "\n",
        "def full_descriptive_features(text):\n",
        "    text = str(text)\n",
        "    doc = nlp(text)\n",
        "    words = [token.text for token in doc if token.is_alpha]\n",
        "    word_lengths = [len(word) for word in words]\n",
        "    sentences = list(doc.sents)\n",
        "    sentence_lengths = [len([token for token in sent if token.is_alpha]) for sent in sentences]\n",
        "    paragraphs = text.split('\\n')\n",
        "    content_words = [token.text for token in doc if token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV']]\n",
        "    function_words = [token.text for token in doc if token.pos_ in ['ADP', 'PRON', 'CONJ', 'DET', 'CCONJ', 'SCONJ']]\n",
        "\n",
        "    return {\n",
        "        'char_count': len(text),\n",
        "        'letter_count': sum(c.isalpha() for c in text),\n",
        "        'word_count': len(words),\n",
        "        'unique_word_count': len(set(words)),\n",
        "        'content_word_count': len(content_words),\n",
        "        'function_word_count': len(function_words),\n",
        "        'sentence_count': len(sentences),\n",
        "        'paragraph_count': len(paragraphs),\n",
        "        'token_count': len(doc),\n",
        "        'min_sentence_length': min(sentence_lengths) if sentence_lengths else 0,\n",
        "        'max_sentence_length': max(sentence_lengths) if sentence_lengths else 0,\n",
        "        'avg_sentence_length': np.mean(sentence_lengths) if sentence_lengths else 0,\n",
        "        'min_word_length': min(word_lengths) if word_lengths else 0,\n",
        "        'max_word_length': max(word_lengths) if word_lengths else 0,\n",
        "        'avg_word_length': np.mean(word_lengths) if word_lengths else 0,\n",
        "        'median_word_length': np.median(word_lengths) if word_lengths else 0,\n",
        "        'std_word_length': np.std(word_lengths) if word_lengths else 0,\n",
        "        'capitalized_word_count': sum(1 for token in doc if token.is_alpha and token.text[0].isupper()),\n",
        "        'punctuation_count': sum(1 for token in doc if token.is_punct),\n",
        "        'comma_count': text.count(','),\n",
        "        'exclamation_count': text.count('!'),\n",
        "        'question_count': text.count('?'),\n",
        "        'period_count': text.count('.'),\n",
        "        'type_token_ratio': len(set(words)) / len(words) if words else 0,\n",
        "        'avg_words_per_paragraph': len(words) / len(paragraphs) if paragraphs else 0,\n",
        "        'avg_chars_per_word': sum(len(word) for word in words) / len(words) if words else 0,\n",
        "        'lexical_density': len(content_words) / len(words) if words else 0\n",
        "    }\n",
        "\n",
        "# Apply\n",
        "descriptive_features = df['text'].apply(full_descriptive_features)\n",
        "descriptive_df = pd.DataFrame(descriptive_features.tolist())\n",
        "result = pd.concat([df, descriptive_df], axis=1)\n",
        "\n",
        "# Save and show\n",
        "result.to_csv(\"Full_descriptive.csv\", index=False)\n",
        "print(\"Done! File saved as Full_data_descriptive.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29x-eNIizJ3-",
        "outputId": "3d6077a8-375d-4aa8-9f7c-3c29627792ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Done! File saved as Full_data_descriptive.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSvEFE7Z6LfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rcdt18gE6PTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Lexical Diversity**"
      ],
      "metadata": {
        "id": "p_mBjC3c6UBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“¦ Install required packages\n",
        "!pip install -q spacy wordfreq lexicalrichness textstat\n",
        "#!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbkaCut96PXH",
        "outputId": "a82b3ba8-e493-4967-a672-1808609a6802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/97.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.8/97.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m939.4/939.4 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for lexicalrichness (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from lexicalrichness import LexicalRichness\n",
        "from wordfreq import word_frequency\n",
        "import textstat\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"sm_data.csv\")\n",
        "\n",
        "# Function to extract 22 lexical diversity metrics\n",
        "def extract_lexical_features(text):\n",
        "    text = str(text)\n",
        "    doc = nlp(text)\n",
        "    words = [token.text for token in doc if token.is_alpha]\n",
        "    lower_words = [w.lower() for w in words]\n",
        "    pos_counts = Counter([token.pos_ for token in doc])\n",
        "    total_words = len(words)\n",
        "    sentences = list(doc.sents)\n",
        "    num_sentences = len(sentences) if sentences else 1\n",
        "\n",
        "    # POS tags\n",
        "    noun_count = pos_counts[\"NOUN\"]\n",
        "    verb_count = pos_counts[\"VERB\"]\n",
        "    adj_count = pos_counts[\"ADJ\"]\n",
        "    adv_count = pos_counts[\"ADV\"]\n",
        "    content_count = noun_count + verb_count + adj_count + adv_count\n",
        "    function_count = total_words - content_count\n",
        "\n",
        "    # Ratios\n",
        "    ttr = len(set(lower_words)) / total_words if total_words else 0\n",
        "    noun_ratio = noun_count / total_words if total_words else 0\n",
        "    verb_ratio = verb_count / total_words if total_words else 0\n",
        "    adj_ratio = adj_count / total_words if total_words else 0\n",
        "    adv_ratio = adv_count / total_words if total_words else 0\n",
        "    content_ratio = content_count / total_words if total_words else 0\n",
        "    function_ratio = function_count / total_words if total_words else 0\n",
        "\n",
        "    # Densities\n",
        "    noun_density = noun_count / num_sentences\n",
        "    verb_density = verb_count / num_sentences\n",
        "    adj_density = adj_count / num_sentences\n",
        "    adv_density = adv_count / num_sentences\n",
        "    content_density = content_count / num_sentences\n",
        "    function_density = function_count / num_sentences\n",
        "\n",
        "    # Avg features\n",
        "    avg_word_len = np.mean([len(w) for w in words]) if words else 0\n",
        "    avg_syllables = np.mean([textstat.syllable_count(w) for w in words]) if words else 0\n",
        "    word_freq = np.mean([word_frequency(w, 'en') for w in words if w.isalpha()]) if words else 0\n",
        "\n",
        "    # Lexical Richness (vocd & MLTD)\n",
        "    lex = LexicalRichness(' '.join(words))\n",
        "    try:\n",
        "        mltd = lex.mltd(threshold=0.72)\n",
        "        vocd = lex.vocab_diversity()\n",
        "    except:\n",
        "        mltd = 0\n",
        "        vocd = 0\n",
        "\n",
        "    # Hapax legomena & dislegomena\n",
        "    freqs = Counter(lower_words)\n",
        "    hapax_legomena = len([w for w, f in freqs.items() if f == 1])\n",
        "    hapax_dislegomena = len([w for w, f in freqs.items() if f == 2])\n",
        "    hapax_legomena_ratio = hapax_legomena / total_words if total_words else 0\n",
        "    hapax_dislegomena_ratio = hapax_dislegomena / total_words if total_words else 0\n",
        "\n",
        "    return {\n",
        "        \"type_token_ratio\": ttr,\n",
        "        \"noun_token_ratio\": noun_ratio,\n",
        "        \"verb_token_ratio\": verb_ratio,\n",
        "        \"adj_token_ratio\": adj_ratio,\n",
        "        \"adv_token_ratio\": adv_ratio,\n",
        "        \"content_word_token_ratio\": content_ratio,\n",
        "        \"function_word_token_ratio\": function_ratio,\n",
        "        \"noun_density\": noun_density,\n",
        "        \"verb_density\": verb_density,\n",
        "        \"adj_density\": adj_density,\n",
        "        \"adv_density\": adv_density,\n",
        "        \"content_word_density\": content_density,\n",
        "        \"function_word_density\": function_density,\n",
        "        \"avg_word_length\": avg_word_len,\n",
        "        \"avg_word_syllables\": avg_syllables,\n",
        "        \"word_frequency_mean\": word_freq,\n",
        "        \"mltd\": mltd,\n",
        "        \"vocd_d\": vocd,\n",
        "        \"hapax_legomena_ratio\": hapax_legomena_ratio,\n",
        "        \"hapax_dislegomena_ratio\": hapax_dislegomena_ratio,\n",
        "        \"open_class_word_ratio\": content_ratio,\n",
        "        \"closed_class_word_ratio\": function_ratio\n",
        "    }\n",
        "\n",
        "# Apply to all rows\n",
        "lexical_features = df['text'].apply(extract_lexical_features)\n",
        "lexical_df = pd.DataFrame(lexical_features.tolist())\n",
        "\n",
        "# Combine and export\n",
        "result = pd.concat([df, lexical_df], axis=1)\n",
        "result.to_csv(\"Full_lexical_diversity.csv\", index=False)\n",
        "print(\"Done! Lexical diversity features saved to 'sm_data_lexical_features.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMjp7nYu6Lt-",
        "outputId": "455523d8-afae-4ddb-eb44-2953b73a3592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Done! Lexical diversity features saved to 'sm_data_lexical_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SjUqcGw368z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Readability Features**"
      ],
      "metadata": {
        "id": "ng4Q9-vqGDgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q textstat lexicalrichness\n"
      ],
      "metadata": {
        "id": "0RbMTnau682o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textstat\n",
        "from lexicalrichness import LexicalRichness\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"sm_data.csv\")\n",
        "\n",
        "# Readability extraction function (7 indicators)\n",
        "def extract_readability_scores(text):\n",
        "    text = str(text)\n",
        "    lex = LexicalRichness(text)\n",
        "\n",
        "    # 1â€“5: From textstat\n",
        "    flesch_grade = textstat.flesch_kincaid_grade(text)\n",
        "    fog_index = textstat.gunning_fog(text)\n",
        "    smog = textstat.smog_index(text)\n",
        "\n",
        "    # 6â€“7: From lexicalrichness or custom\n",
        "    try:\n",
        "        brunet_index = lex.brunet_index()\n",
        "    except:\n",
        "        brunet_index = 0\n",
        "\n",
        "    try:\n",
        "        honore_stat = lex.honore_stat()\n",
        "    except:\n",
        "        honore_stat = 0\n",
        "\n",
        "    # Szigriszt-Pazos Perspicuity Index (approximation)\n",
        "    total_words = textstat.lexicon_count(text, removepunct=True)\n",
        "    syllables = textstat.syllable_count(text)\n",
        "    sentences = textstat.sentence_count(text)\n",
        "    try:\n",
        "        szigriszt = 206.835 - (62.3 * (syllables / total_words)) - (sentences / total_words * 100)\n",
        "    except:\n",
        "        szigriszt = 0\n",
        "\n",
        "    # Readability mean Âµ\n",
        "    scores = [flesch_grade, fog_index, smog, brunet_index, honore_stat, szigriszt]\n",
        "    readability_mean = np.mean([s for s in scores if s > 0]) if scores else 0\n",
        "\n",
        "    return {\n",
        "        \"flesch_kincaid_grade\": flesch_grade,\n",
        "        \"gunning_fog_index\": fog_index,\n",
        "        \"smog_index\": smog,\n",
        "        \"brunet_index\": brunet_index,\n",
        "        \"honore_statistic\": honore_stat,\n",
        "        \"szigriszt_pazos_index\": szigriszt,\n",
        "        \"readability_mean_score\": readability_mean\n",
        "    }\n",
        "\n",
        "# Apply to all rows\n",
        "readability_features = df['text'].apply(extract_readability_scores)\n",
        "readability_df = pd.DataFrame(readability_features.tolist())\n",
        "\n",
        "# âž• Combine and save\n",
        "result = pd.concat([df, readability_df], axis=1)\n",
        "result.to_csv(\"Full_readability_features.csv\", index=False)\n",
        "print(\" Done! Saved to 'sm_data_readability_features.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zebAEZ9Z69XN",
        "outputId": "305c46f0-7e2d-43e5-9c99-849d246d36c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Done! Saved to 'sm_data_readability_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JNil0Qsa69a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yHJVWLio8FHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Syntactic Complexity**"
      ],
      "metadata": {
        "id": "oHto2TGO8F0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "41p0wuD59bqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from nltk.metrics import edit_distance\n",
        "from collections import Counter\n",
        "from math import log2\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load your CSV\n",
        "df = pd.read_csv(\"sm_data.csv\")\n",
        "\n",
        "# Helper function: Shannon entropy\n",
        "def shannon_entropy(freqs):\n",
        "    total = sum(freqs.values())\n",
        "    return -sum((f / total) * log2(f / total) for f in freqs.values() if f > 0)\n",
        "\n",
        "# Helper function: Count clauses\n",
        "def count_clauses(sent):\n",
        "    return sum(1 for token in sent if token.dep_ in [\"ccomp\", \"advcl\", \"relcl\", \"xcomp\", \"acl\", \"conj\", \"parataxis\"])\n",
        "\n",
        "# Main function for syntactic complexity\n",
        "def syntactic_complexity(text):\n",
        "    text = str(text)\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    clause_counts = [count_clauses(sent) + 1 for sent in sentences]  # base clause\n",
        "    clause_bins = [0] * 7\n",
        "    for count in clause_counts:\n",
        "        idx = min(count, 7) - 1\n",
        "        clause_bins[idx] += 1\n",
        "    total_sents = len(sentences) or 1\n",
        "    clause_props = [b / total_sents for b in clause_bins]\n",
        "\n",
        "    pos_tags = [token.pos_ for token in doc if not token.is_punct]\n",
        "    lemmas = [token.lemma_ for token in doc if not token.is_punct]\n",
        "    pos_entropy = shannon_entropy(Counter(pos_tags))\n",
        "    lemma_entropy = shannon_entropy(Counter(lemmas))\n",
        "\n",
        "    tree_depths = [abs(token.head.i - token.i) for token in doc if token.dep_ != \"punct\"]\n",
        "    avg_tree_depth = np.mean(tree_depths) if tree_depths else 0\n",
        "    avg_clause_length = np.mean(clause_counts) if clause_counts else 0\n",
        "\n",
        "    # Edit distance using lemma sentences\n",
        "    lemma_sents = [[token.lemma_ for token in sent if not token.is_punct] for sent in sentences]\n",
        "    edit_dists = []\n",
        "    for i in range(len(lemma_sents) - 1):\n",
        "        s1 = \" \".join(lemma_sents[i])\n",
        "        s2 = \" \".join(lemma_sents[i + 1])\n",
        "        dist = edit_distance(s1, s2) if s1 and s2 else 0\n",
        "        edit_dists.append(dist)\n",
        "    avg_edit_distance = np.mean(edit_dists) if edit_dists else 0\n",
        "\n",
        "    return {\n",
        "        \"prop_clause_1\": clause_props[0],\n",
        "        \"prop_clause_2\": clause_props[1],\n",
        "        \"prop_clause_3\": clause_props[2],\n",
        "        \"prop_clause_4\": clause_props[3],\n",
        "        \"prop_clause_5\": clause_props[4],\n",
        "        \"prop_clause_6\": clause_props[5],\n",
        "        \"prop_clause_7_plus\": clause_props[6],\n",
        "        \"avg_sentence_clause_length\": avg_clause_length,\n",
        "        \"avg_tree_depth\": avg_tree_depth,\n",
        "        \"pos_tag_entropy\": pos_entropy,\n",
        "        \"lemma_entropy\": lemma_entropy,\n",
        "        \"avg_edit_distance_between_sentences\": avg_edit_distance\n",
        "    }\n",
        "\n",
        "# Apply to all rows\n",
        "syntax_features = df['text'].apply(syntactic_complexity)\n",
        "syntax_df = pd.DataFrame(syntax_features.tolist())\n",
        "\n",
        "# Merge and export\n",
        "result = pd.concat([df, syntax_df], axis=1)\n",
        "result.to_csv(\"Full_syntactic_features.csv\", index=False)\n",
        "print(\" Done! File saved as 'sm_data_syntactic_features.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etnBPyid9XP7",
        "outputId": "cb48564c-8148-4f8d-9bb6-d9ad8ce0d42e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Done! File saved as 'sm_data_syntactic_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2LJNNfw_7uzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G_OxWm3J-Ffw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.PSYCHOLINGUISTIC FEATURES**"
      ],
      "metadata": {
        "id": "2Kay1bJN-J-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "BVf5Iezd-Xtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Step 1: Download raw .dct file\n",
        "url = \"https://raw.githubusercontent.com/samzhang111/mrc-psycholinguistics/refs/heads/master/mrc2.dct\"\n",
        "r = requests.get(url)\n",
        "lines = r.text.splitlines()\n",
        "\n",
        "# Step 2: Parse fixed-width fields (word and familiarity)\n",
        "familiarity_data = []\n",
        "for line in lines:\n",
        "    word = line[0:20].strip()\n",
        "    fam = int(line[104:109].strip()) if line[104:109].strip().isdigit() else None\n",
        "    if fam:\n",
        "        familiarity_data.append((word.lower(), fam))\n",
        "\n",
        "# Step 3: Save to CSV\n",
        "familiarity_df = pd.DataFrame(familiarity_data, columns=[\"word\", \"familiarity\"])\n",
        "familiarity_df.to_csv(\"familiarity.csv\", index=False)\n",
        "\n",
        "print(\" familiarity.csv created with\", len(familiarity_df), \"words\")\n",
        "familiarity_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "4j_aHzOrJ2f6",
        "outputId": "e230f32b-edf6-4413-8492-a27ba19602cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… familiarity.csv created with 297 words\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   word  familiarity\n",
              "0  14135001610906500018           20\n",
              "1  14155000000000000000         2000\n",
              "2  14136000000000000000          200\n",
              "3  14136000000000000000          200\n",
              "4  15146000000000000000          200"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a37296e9-0fa1-4fca-be11-cddca28e4b8d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>familiarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14135001610906500018</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>14155000000000000000</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14136000000000000000</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14136000000000000000</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15146000000000000000</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a37296e9-0fa1-4fca-be11-cddca28e4b8d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a37296e9-0fa1-4fca-be11-cddca28e4b8d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a37296e9-0fa1-4fca-be11-cddca28e4b8d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1e2b25e3-20d8-40db-aed3-94150605ecee\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1e2b25e3-20d8-40db-aed3-94150605ecee')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1e2b25e3-20d8-40db-aed3-94150605ecee button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "familiarity_df",
              "summary": "{\n  \"name\": \"familiarity_df\",\n  \"rows\": 297,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 107,\n        \"samples\": [\n          \"14135000060200300000\",\n          \"15137000000000000000\",\n          \"14135000020200200001\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"familiarity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5304,\n        \"min\": 2,\n        \"max\": 21020,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          20,\n          2000,\n          10102\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“¦ Install required library\n",
        "!pip install -q pandas requests\n"
      ],
      "metadata": {
        "id": "MLS2XlK5SkCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Step 1: Download valence-arousal dataset (X-ANEW / Warriner et al.)\n",
        "url = \"https://raw.githubusercontent.com/JULIELab/XANEW/master/Ratings_Warriner_et_al.csv\"\n",
        "r = requests.get(url)\n",
        "if r.status_code != 200:\n",
        "    raise Exception(f\"Download failed with status code {r.status_code}\")\n",
        "\n",
        "# Step 2: Load into DataFrame\n",
        "from io import StringIO\n",
        "df = pd.read_csv(StringIO(r.text))\n",
        "\n",
        "# We expect columns: Word, V.Mean.Sum, A.Mean.Sum, D.Mean.Sum\n",
        "needed = [\"Word\", \"V.Mean.Sum\", \"A.Mean.Sum\"]\n",
        "for col in needed:\n",
        "    if col not in df.columns:\n",
        "        raise Exception(f\"Column {col} missing in downloaded file\")\n",
        "\n",
        "# Step 3: Clean & extract just word, valence, arousal\n",
        "val_df = df[[\"Word\", \"V.Mean.Sum\", \"A.Mean.Sum\"]].copy()\n",
        "val_df.columns = [\"word\", \"valence\", \"arousal\"]\n",
        "val_df[\"word\"] = val_df[\"word\"].str.lower()\n",
        "\n",
        "# Step 4: Save as CSV\n",
        "val_df.to_csv(\"valence.csv\", index=False)\n",
        "print(f\" Created valence.csv with {len(val_df)} entries\")\n",
        "\n",
        "# Preview\n",
        "val_df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "O7RVDNGwSkFX",
        "outputId": "a567bed6-e517-4e5a-84e8-3ded7d4e31d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Created valence.csv with 13915 entries\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          word  valence  arousal\n",
              "0     aardvark     6.26     2.41\n",
              "1      abalone     5.30     2.65\n",
              "2      abandon     2.84     3.73\n",
              "3  abandonment     2.63     4.95\n",
              "4        abbey     5.85     2.20"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18e2770f-7d47-4431-a0bb-8b542118774e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>valence</th>\n",
              "      <th>arousal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>aardvark</td>\n",
              "      <td>6.26</td>\n",
              "      <td>2.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>abalone</td>\n",
              "      <td>5.30</td>\n",
              "      <td>2.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>abandon</td>\n",
              "      <td>2.84</td>\n",
              "      <td>3.73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>abandonment</td>\n",
              "      <td>2.63</td>\n",
              "      <td>4.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>abbey</td>\n",
              "      <td>5.85</td>\n",
              "      <td>2.20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18e2770f-7d47-4431-a0bb-8b542118774e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-18e2770f-7d47-4431-a0bb-8b542118774e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-18e2770f-7d47-4431-a0bb-8b542118774e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2f00cb37-60a4-49fc-83e1-119ffbb831d3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2f00cb37-60a4-49fc-83e1-119ffbb831d3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2f00cb37-60a4-49fc-83e1-119ffbb831d3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "val_df",
              "summary": "{\n  \"name\": \"val_df\",\n  \"rows\": 13915,\n  \"fields\": [\n    {\n      \"column\": \"word\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13904,\n        \"samples\": [\n          \"join\",\n          \"witless\",\n          \"bottling\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2748915511901322,\n        \"min\": 1.26,\n        \"max\": 8.53,\n        \"num_unique_values\": 605,\n        \"samples\": [\n          7.64,\n          4.28,\n          2.68\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"arousal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8964130718330646,\n        \"min\": 1.6,\n        \"max\": 7.79,\n        \"num_unique_values\": 471,\n        \"samples\": [\n          3.0,\n          4.77,\n          3.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n"
      ],
      "metadata": {
        "id": "o3CxCdZ2ecfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from functools import reduce\n",
        "\n",
        "# Load spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load tweet dataset\n",
        "df = pd.read_csv(\"sm_data.csv\")  # assumes column is named 'text'\n",
        "\n",
        "# Load psycholinguistic norm datasets\n",
        "concreteness_df = pd.read_csv(\"concreteness.csv\").rename(columns={\"word\": \"Word\", \"conc_mean\": \"Concreteness\"})\n",
        "imageability_df = pd.read_csv(\"imageability.csv\").rename(columns={\"word\": \"Word\", \"imgability\": \"Imageability\"})\n",
        "familiarity_df = pd.read_csv(\"familiarity.csv\").rename(columns={\"word\": \"Word\", \"familiarity\": \"Familiarity\"})\n",
        "aoa_df = pd.read_csv(\"aoa.csv\").rename(columns={\"word\": \"Word\", \"aoa\": \"AoA\"})\n",
        "valence_df = pd.read_csv(\"valence.csv\").rename(columns={\"word\": \"Word\"})\n",
        "\n",
        "# Ensure all 'Word' columns are lowercase strings\n",
        "for df_norm in [concreteness_df, imageability_df, familiarity_df, aoa_df, valence_df]:\n",
        "    df_norm[\"Word\"] = df_norm[\"Word\"].astype(str).str.lower()\n",
        "\n",
        "# Merge all norms into one DataFrame\n",
        "norms = reduce(lambda left, right: pd.merge(left, right, on=\"Word\", how=\"outer\"),\n",
        "               [concreteness_df, imageability_df, familiarity_df, aoa_df, valence_df])\n",
        "\n",
        "# Helper to compute feature statistics\n",
        "def psych_stats(values):\n",
        "    return {\n",
        "        \"mean\": np.mean(values) if values else 0,\n",
        "        \"min\": np.min(values) if values else 0,\n",
        "        \"max\": np.max(values) if values else 0,\n",
        "        \"std\": np.std(values) if values else 0,\n",
        "        \"median\": np.median(values) if values else 0,\n",
        "    }\n",
        "\n",
        "# Main extractor for a single tweet\n",
        "def extract_psycholinguistics(text):\n",
        "    doc = nlp(str(text))\n",
        "    words = [token.text.lower() for token in doc if token.is_alpha]\n",
        "    word_data = norms[norms[\"Word\"].isin(words)]\n",
        "\n",
        "    result = {}\n",
        "    for feature in [\"Concreteness\", \"Imageability\", \"Familiarity\", \"AoA\", \"valence\", \"arousal\"]:\n",
        "        stats = psych_stats(word_data[feature].dropna().tolist())\n",
        "        for k, v in stats.items():\n",
        "            result[f\"{feature.lower()}_{k}\"] = v\n",
        "    return result\n",
        "\n",
        "# Apply feature extractor to each tweet\n",
        "psych_features = df[\"text\"].apply(extract_psycholinguistics)\n",
        "psych_df = pd.DataFrame(psych_features.tolist())\n",
        "\n",
        "# Combine and export enriched dataset\n",
        "result = pd.concat([df, psych_df], axis=1)\n",
        "result.to_csv(\"Full_psycholinguistic_features.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Done! 30 psycholinguistic features saved to 'sm_data_psycholinguistic_features.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK8hQg3xgNLu",
        "outputId": "df52e2f5-2429-47c0-d7e4-8295cb4ddcb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Done! 30 psycholinguistic features saved to 'sm_data_psycholinguistic_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDPBfvAZjc05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nkK5dd6Njc3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.WORD_INFORMATION_FEATURES**"
      ],
      "metadata": {
        "id": "nIDNK1g0koeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"sm_data.csv\")  # assumes column name is 'text'\n",
        "\n",
        "# Feature extractor\n",
        "def extract_word_info(text):\n",
        "    doc = nlp(str(text))\n",
        "    tokens = [token for token in doc if token.is_alpha]\n",
        "    total_tokens = len(tokens)\n",
        "    total_chars = sum(len(token.text) for token in tokens)\n",
        "    stopwords = sum(1 for token in tokens if token.is_stop)\n",
        "    puncts = sum(1 for token in doc if token.is_punct)\n",
        "\n",
        "    pos_counts = {\n",
        "        \"noun\": 0, \"verb\": 0, \"adj\": 0, \"adv\": 0, \"pron\": 0,\n",
        "        \"propn\": 0, \"intj\": 0, \"conj\": 0, \"part\": 0, \"sym\": 0, \"num\": 0,\n",
        "        \"content_words\": 0, \"function_words\": 0\n",
        "    }\n",
        "\n",
        "    for token in tokens:\n",
        "        pos = token.pos_.lower()\n",
        "        if pos in pos_counts:\n",
        "            pos_counts[pos] += 1\n",
        "        if pos in [\"noun\", \"verb\", \"adj\", \"adv\"]:\n",
        "            pos_counts[\"content_words\"] += 1\n",
        "        else:\n",
        "            pos_counts[\"function_words\"] += 1\n",
        "\n",
        "    result = {\n",
        "        \"token_count\": total_tokens,\n",
        "        \"char_count\": total_chars,\n",
        "        \"avg_word_length\": total_chars / total_tokens if total_tokens > 0 else 0,\n",
        "        \"stopword_ratio\": stopwords / total_tokens if total_tokens > 0 else 0,\n",
        "        \"punctuation_count\": puncts,\n",
        "        \"punctuation_ratio\": puncts / len(doc) if len(doc) > 0 else 0,\n",
        "        \"avg_sentence_length\": sum(len(sent) for sent in doc.sents) / len(list(doc.sents)) if len(list(doc.sents)) > 0 else 0,\n",
        "        \"lexical_density\": pos_counts[\"content_words\"] / total_tokens if total_tokens > 0 else 0,\n",
        "    }\n",
        "\n",
        "    for tag, count in pos_counts.items():\n",
        "        result[f\"num_{tag}\"] = count\n",
        "        result[f\"{tag}_ratio\"] = count / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "    return result\n",
        "\n",
        "# Apply extraction\n",
        "word_info_features = df[\"text\"].apply(extract_word_info)\n",
        "word_info_df = pd.DataFrame(word_info_features.tolist())\n",
        "\n",
        "# Save result\n",
        "result = pd.concat([df, word_info_df], axis=1)\n",
        "result.to_csv(\"Full_word_information_features.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Done! All 24 word-level features saved to 'sm_data_word_information_features.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbZjTHZNjc62",
        "outputId": "5f651ac8-1747-4a61-bb51-8ba94bd1e041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Done! All 24 word-level features saved to 'sm_data_word_information_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgBRuD8ljc9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-xf5c4delCe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.Referential cohesion features**"
      ],
      "metadata": {
        "id": "As_X7DDRlIpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Load resources\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"sm_data.csv\")  # must contain 'text' column\n",
        "\n",
        "# Helper: extract nouns, arguments, stems, content words, anaphors\n",
        "def extract_features_from_sent(sent):\n",
        "    tokens = [t for t in sent if t.is_alpha]\n",
        "    nouns = set([t.lemma_.lower() for t in tokens if t.pos_ == \"NOUN\"])\n",
        "    arguments = set([t.lemma_.lower() for t in tokens if t.dep_ in [\"nsubj\", \"dobj\", \"pobj\"]])\n",
        "    stems = set([stemmer.stem(t.text.lower()) for t in tokens])\n",
        "    content_words = set([t.lemma_.lower() for t in tokens if t.pos_ in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]])\n",
        "    anaphors = set([t.lemma_.lower() for t in tokens if t.pos_ == \"PRON\"])\n",
        "    all_words = set([t.lemma_.lower() for t in tokens])\n",
        "    return nouns, arguments, stems, content_words, anaphors, all_words\n",
        "\n",
        "# Overlap calculator\n",
        "def compute_overlap(curr, prev):\n",
        "    return len(curr & prev) / len(curr | prev) if curr | prev else 0\n",
        "\n",
        "# Main extractor for referential cohesion\n",
        "def extract_referential_cohesion(text):\n",
        "    doc = nlp(str(text))\n",
        "    sents = list(doc.sents)\n",
        "\n",
        "    # Lists of feature sets per sentence\n",
        "    fsets = [extract_features_from_sent(sent) for sent in sents]\n",
        "\n",
        "    local_scores = defaultdict(list)\n",
        "    global_scores = defaultdict(list)\n",
        "\n",
        "    for i in range(1, len(fsets)):\n",
        "        for name, idx in zip([\"noun\", \"arg\", \"stem\", \"content\", \"anaphor\", \"all\"],\n",
        "                             range(6)):\n",
        "            curr = fsets[i][idx]\n",
        "            prev = fsets[i - 1][idx]\n",
        "            # local overlap\n",
        "            local_scores[f\"{name}_local\"].append(compute_overlap(curr, prev))\n",
        "\n",
        "            # global overlap (vs all previous)\n",
        "            global_union = set().union(*[fsets[j][idx] for j in range(i)])\n",
        "            global_scores[f\"{name}_global\"].append(compute_overlap(curr, global_union))\n",
        "\n",
        "    # Compute mean overlaps\n",
        "    result = {}\n",
        "    for name in [\"noun\", \"arg\", \"stem\", \"content\", \"anaphor\", \"all\"]:\n",
        "        result[f\"{name}_overlap_local\"] = sum(local_scores[f\"{name}_local\"]) / len(local_scores[f\"{name}_local\"]) if local_scores[f\"{name}_local\"] else 0\n",
        "        result[f\"{name}_overlap_global\"] = sum(global_scores[f\"{name}_global\"]) / len(global_scores[f\"{name}_global\"]) if global_scores[f\"{name}_global\"] else 0\n",
        "\n",
        "    return result\n",
        "\n",
        "# Apply to dataset\n",
        "cohesion_features = df[\"text\"].apply(extract_referential_cohesion)\n",
        "cohesion_df = pd.DataFrame(cohesion_features.tolist())\n",
        "\n",
        "# Combine and save\n",
        "final_df = pd.concat([df, cohesion_df], axis=1)\n",
        "final_df.to_csv(\"Full_referential_cohesion.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Referential cohesion features (12) saved to 'sm_data_referential_cohesion.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3Grbg_6lChm",
        "outputId": "a9f3955f-3edf-4433-cec9-8b326e181e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Referential cohesion features (12) saved to 'sm_data_referential_cohesion.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XkW4Gs2lVDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pCjMwwRFlVGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8.Textual simplicity features**"
      ],
      "metadata": {
        "id": "GFdXxxddlmQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"sm_data.csv\")  # must contain 'text' column\n",
        "\n",
        "# Simplicity extractor\n",
        "def extract_textual_simplicity(text):\n",
        "    doc = nlp(str(text))\n",
        "    sentences = list(doc.sents)\n",
        "    total = len(sentences)\n",
        "    short = medium = long = very_long = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        length = len([token for token in sent if token.is_alpha or token.is_digit])\n",
        "        if length <= 10:\n",
        "            short += 1\n",
        "        elif 11 <= length <= 20:\n",
        "            medium += 1\n",
        "        elif 21 <= length <= 30:\n",
        "            long += 1\n",
        "        else:\n",
        "            very_long += 1\n",
        "\n",
        "    return {\n",
        "        \"short_sent_ratio\": short / total if total else 0,\n",
        "        \"medium_sent_ratio\": medium / total if total else 0,\n",
        "        \"long_sent_ratio\": long / total if total else 0,\n",
        "        \"very_long_sent_ratio\": very_long / total if total else 0,\n",
        "    }\n",
        "\n",
        "# Apply to dataset\n",
        "simplicity_features = df[\"text\"].apply(extract_textual_simplicity)\n",
        "simplicity_df = pd.DataFrame(simplicity_features.tolist())\n",
        "\n",
        "# Combine and save\n",
        "final_df = pd.concat([df, simplicity_df], axis=1)\n",
        "final_df.to_csv(\"Full_textual_simplicity.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Textual simplicity features (4) saved to 'sm_data_textual_simplicity.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxeysjIclVKa",
        "outputId": "e6b53760-9e05-4ef8-a978-5177b82919f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Textual simplicity features (4) saved to 'sm_data_textual_simplicity.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYqMwMJslVNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EWkCyvW-luXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.Semantic cohesion features**"
      ],
      "metadata": {
        "id": "cP6fe0Vdl-WT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!python -m spacy download en_core_web_md\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0r6ci820mZdn",
        "outputId": "1b1d95a9-0f5e-4985-f4b5-ee08688bbdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.8.0\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Load medium spaCy model (has word vectors)\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"sm_data.csv\")  # ensure 'text' column exists\n",
        "\n",
        "# Utility: average similarity between consecutive items\n",
        "def pairwise_similarity(vectors):\n",
        "    if len(vectors) < 2:\n",
        "        return 0.0, 0.0\n",
        "    sims = [cosine_similarity([vectors[i]], [vectors[i+1]])[0][0] for i in range(len(vectors)-1)]\n",
        "    return np.mean(sims), np.std(sims)\n",
        "\n",
        "# Semantic Cohesion extractor\n",
        "def extract_semantic_cohesion(text):\n",
        "    doc = nlp(str(text))\n",
        "\n",
        "    # Sentences\n",
        "    sents = [sent.text.strip() for sent in doc.sents if len(sent) > 3]\n",
        "    sent_vecs = [nlp(sent).vector for sent in sents if nlp(sent).has_vector]\n",
        "\n",
        "    # All sentence pair similarity\n",
        "    all_sims = []\n",
        "    for i in range(len(sent_vecs)):\n",
        "        for j in range(i + 1, len(sent_vecs)):\n",
        "            sim = cosine_similarity([sent_vecs[i]], [sent_vecs[j]])[0][0]\n",
        "            all_sims.append(sim)\n",
        "\n",
        "    # Paragraphs\n",
        "    paras = [p.strip() for p in text.split(\"\\n\") if len(p.strip().split()) > 3]\n",
        "    para_vecs = [nlp(p).vector for p in paras if nlp(p).has_vector]\n",
        "\n",
        "    # Pairwise sentence and paragraph similarities\n",
        "    sent_adj_mean, sent_adj_std = pairwise_similarity(sent_vecs)\n",
        "    para_adj_mean, para_adj_std = pairwise_similarity(para_vecs)\n",
        "\n",
        "    result = {\n",
        "        \"lsa_sent_adj_mean\": sent_adj_mean,\n",
        "        \"lsa_sent_adj_std\": sent_adj_std,\n",
        "        \"lsa_sent_all_mean\": np.mean(all_sims) if all_sims else 0,\n",
        "        \"lsa_sent_all_std\": np.std(all_sims) if all_sims else 0,\n",
        "        \"lsa_para_adj_mean\": para_adj_mean,\n",
        "        \"lsa_para_adj_std\": para_adj_std,\n",
        "        \"lsa_first_last_para_similarity\": cosine_similarity([para_vecs[0]], [para_vecs[-1]])[0][0] if len(para_vecs) > 1 else 0,\n",
        "        \"lsa_first_last_sent_similarity\": cosine_similarity([sent_vecs[0]], [sent_vecs[-1]])[0][0] if len(sent_vecs) > 1 else 0,\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Apply to all texts\n",
        "semantic_features = df[\"text\"].apply(extract_semantic_cohesion)\n",
        "semantic_df = pd.DataFrame(semantic_features.tolist())\n",
        "\n",
        "# Combine and export\n",
        "final_df = pd.concat([df, semantic_df], axis=1)\n",
        "final_df.to_csv(\"Full_semantic_cohesion.csv\", index=False)\n",
        "print(\"Semantic cohesion features (8) saved to 'sm_data_semantic_cohesion.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujML07b3luak",
        "outputId": "7ab39983-d069-468c-beb8-382c0b3f70b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Semantic cohesion features (8) saved to 'sm_data_semantic_cohesion.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MG2htnpflud4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dUTkCVd2n3wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.Word frequency features**"
      ],
      "metadata": {
        "id": "_O1YkV-Rn_Sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from wordfreq import zipf_frequency\n",
        "\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "df = pd.read_csv(\"sm_data.csv\")\n",
        "\n",
        "def extract_word_frequency(text):\n",
        "    doc = nlp(str(text))\n",
        "    tokens = [token for token in doc if token.is_alpha and not token.is_stop]\n",
        "\n",
        "    freqs = []\n",
        "    pos_freqs = {'NOUN': [], 'VERB': [], 'ADJ': [], 'ADV': []}\n",
        "    rare_counts = {'NOUN': 0, 'VERB': 0, 'ADJ': 0, 'ADV': 0}\n",
        "\n",
        "    for token in tokens:\n",
        "        word = token.text.lower()\n",
        "        freq = zipf_frequency(word, 'en')\n",
        "        freqs.append(freq)\n",
        "\n",
        "        if token.pos_ in pos_freqs:\n",
        "            pos_freqs[token.pos_].append(freq)\n",
        "            if freq < 2.0:\n",
        "                rare_counts[token.pos_] += 1\n",
        "\n",
        "    rare_total = sum(1 for f in freqs if f < 2.0)\n",
        "    content_freqs = pos_freqs['NOUN'] + pos_freqs['VERB'] + pos_freqs['ADJ'] + pos_freqs['ADV']\n",
        "    rare_content = sum(rare_counts.values())\n",
        "\n",
        "    result = {\n",
        "        \"mean_zipf\": np.mean(freqs) if freqs else 0,\n",
        "        \"rare_word_count\": rare_total,\n",
        "        \"rare_noun_count\": rare_counts['NOUN'],\n",
        "        \"rare_verb_count\": rare_counts['VERB'],\n",
        "        \"rare_adj_count\": rare_counts['ADJ'],\n",
        "        \"rare_adv_count\": rare_counts['ADV'],\n",
        "        \"rare_content_word_count\": rare_content,\n",
        "        \"content_zipf_mean\": np.mean(content_freqs) if content_freqs else 0,\n",
        "        \"noun_zipf_mean\": np.mean(pos_freqs['NOUN']) if pos_freqs['NOUN'] else 0,\n",
        "        \"verb_zipf_mean\": np.mean(pos_freqs['VERB']) if pos_freqs['VERB'] else 0,\n",
        "        \"adj_zipf_mean\": np.mean(pos_freqs['ADJ']) if pos_freqs['ADJ'] else 0,\n",
        "        \"adv_zipf_mean\": np.mean(pos_freqs['ADV']) if pos_freqs['ADV'] else 0,\n",
        "        \"word_count\": len(tokens),\n",
        "        \"content_word_count\": len(content_freqs),\n",
        "        \"rare_ratio\": rare_total / len(tokens) if tokens else 0\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "word_freq_features = df[\"text\"].apply(extract_word_frequency)\n",
        "word_freq_df = pd.DataFrame(word_freq_features.tolist())\n",
        "\n",
        "# Combine and save\n",
        "result = pd.concat([df, word_freq_df], axis=1)\n",
        "result.to_csv(\"Full_word_frequency_features.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Word frequency features (16) saved to 'sm_data_word_frequency_features.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-82HieBon30U",
        "outputId": "9747c6e8-58ab-4c16-cfaa-fd5f0b57c5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Word frequency features (16) saved to 'sm_data_word_frequency_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_8p9vDdoKCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_-W-n5OAoKFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**11.Syntactic Pattern Density**"
      ],
      "metadata": {
        "id": "S9lpWUtoo5-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load model and data\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "df = pd.read_csv(\"sm_data.csv\")\n",
        "\n",
        "# List of subordinating conjunctions (non-exhaustive)\n",
        "sub_conjs = {\"because\", \"although\", \"since\", \"though\", \"if\", \"when\", \"while\", \"unless\", \"whereas\"}\n",
        "\n",
        "# Syntactic feature extractor\n",
        "def extract_syntactic_density(text):\n",
        "    doc = nlp(str(text))\n",
        "    sents = list(doc.sents)\n",
        "    num_sents = len(sents)\n",
        "    num_tokens = len([t for t in doc if not t.is_space])\n",
        "\n",
        "    noun_phrases = len(list(doc.noun_chunks))\n",
        "    verb_phrases = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
        "    negations = sum(1 for token in doc if token.lower_ in {\"not\", \"n't\", \"never\", \"no\"} or token.dep_ == \"neg\")\n",
        "    coord_conj = sum(1 for token in doc if token.dep_ == \"cc\")\n",
        "    subord_conj = sum(1 for token in doc if token.text.lower() in sub_conjs)\n",
        "    prepositions = sum(1 for token in doc if token.pos_ == \"ADP\")\n",
        "    relative_clauses = sum(1 for token in doc if token.dep_ == \"relcl\")\n",
        "    auxiliaries = sum(1 for token in doc if token.dep_ == \"aux\")\n",
        "    adj_clauses = sum(1 for token in doc if token.dep_ == \"acl\")\n",
        "    adv_clauses = sum(1 for token in doc if token.dep_ == \"advcl\")\n",
        "    appositives = sum(1 for token in doc if token.dep_ == \"appos\")\n",
        "\n",
        "    return {\n",
        "        \"sentence_count\": num_sents,\n",
        "        \"token_count\": num_tokens,\n",
        "        \"avg_sentence_length\": num_tokens / num_sents if num_sents > 0 else 0,\n",
        "        \"noun_phrase_density\": noun_phrases / num_sents if num_sents else 0,\n",
        "        \"verb_phrase_density\": verb_phrases / num_sents if num_sents else 0,\n",
        "        \"negation_density\": negations / num_sents if num_sents else 0,\n",
        "        \"coord_conj_density\": coord_conj / num_sents if num_sents else 0,\n",
        "        \"subord_conj_density\": subord_conj / num_sents if num_sents else 0,\n",
        "        \"preposition_density\": prepositions / num_sents if num_sents else 0,\n",
        "        \"relative_clause_density\": relative_clauses / num_sents if num_sents else 0,\n",
        "        \"auxiliary_density\": auxiliaries / num_sents if num_sents else 0,\n",
        "        \"adjective_clause_density\": adj_clauses / num_sents if num_sents else 0,\n",
        "        \"adverbial_clause_density\": adv_clauses / num_sents if num_sents else 0,\n",
        "        \"appositive_density\": appositives / num_sents if num_sents else 0\n",
        "    }\n",
        "\n",
        "# Apply to data\n",
        "syntactic_features = df[\"text\"].apply(extract_syntactic_density)\n",
        "syntactic_df = pd.DataFrame(syntactic_features.tolist())\n",
        "\n",
        "# Combine and export\n",
        "final_df = pd.concat([df, syntactic_df], axis=1)\n",
        "final_df.to_csv(\"Full_syntactic_density_features.csv\", index=False)\n",
        "\n",
        "print(\"âœ… Syntactic Pattern Density (14 features) saved to 'sm_data_syntactic_density_features.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6t0auLEoKJd",
        "outputId": "19eff27d-e16a-4e87-9d05-0b206fd6f2f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Syntactic Pattern Density (14 features) saved to 'sm_data_syntactic_density_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEvifOKMp7_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**12.Connective features**"
      ],
      "metadata": {
        "id": "9EEbEzoXp_3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load spaCy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"sm_data.csv\")\n",
        "\n",
        "# Define connective categories\n",
        "connectives = {\n",
        "    \"causal\": {\"because\", \"since\", \"as\", \"so\"},\n",
        "    \"logical\": {\"if\", \"then\", \"therefore\", \"thus\"},\n",
        "    \"adversative\": {\"but\", \"however\", \"although\", \"though\", \"nevertheless\", \"nonetheless\"},\n",
        "    \"temporal\": {\"before\", \"after\", \"when\", \"while\", \"then\"},\n",
        "    \"additive\": {\"and\", \"also\", \"moreover\", \"in addition\", \"furthermore\"}\n",
        "}\n",
        "\n",
        "# Extract function\n",
        "def extract_connectives(text):\n",
        "    doc = nlp(str(text))\n",
        "    words = [token.text.lower() for token in doc if token.is_alpha]\n",
        "\n",
        "    counts = {k: 0 for k in connectives}\n",
        "\n",
        "    for word in words:\n",
        "        for conn_type, conn_words in connectives.items():\n",
        "            if word in conn_words:\n",
        "                counts[conn_type] += 1\n",
        "\n",
        "    total = sum(counts.values())\n",
        "    result = {\n",
        "        \"causal_connectives\": counts[\"causal\"],\n",
        "        \"logical_connectives\": counts[\"logical\"],\n",
        "        \"adversative_connectives\": counts[\"adversative\"],\n",
        "        \"temporal_connectives\": counts[\"temporal\"],\n",
        "        \"additive_connectives\": counts[\"additive\"],\n",
        "        \"all_connectives\": total\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Apply to dataset\n",
        "connective_features = df[\"text\"].apply(extract_connectives)\n",
        "connective_df = pd.DataFrame(connective_features.tolist())\n",
        "\n",
        "# Combine and save\n",
        "final_df = pd.concat([df, connective_df], axis=1)\n",
        "final_df.to_csv(\"Full_connective_features.csv\", index=False)\n",
        "\n",
        "print(\"Connective features (6) saved to 'sm_data_connective_features.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRrqQvk8p8B_",
        "outputId": "6ab65a64-9609-45cc-abe6-a4eb5dbca85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Connective features (6) saved to 'sm_data_connective_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sOtxCt_HvBgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LB8C8TYXvMfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EMNLP-STYLE COMPLEXITY FEATURES**"
      ],
      "metadata": {
        "id": "Z4vMzJyNvFnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“¦ Install required packages\n",
        "!pip install -q spacy textstat syllapy scikit-learn\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA-tMJ0FvQhu",
        "outputId": "4ca1e3bc-9584-4d54-a710-5ee22eec203d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import syllapy\n",
        "from textstat import dale_chall_readability_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "from math import log2\n",
        "\n",
        "#  Load model and data\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "df = pd.read_csv(\"sm_data.csv\")  # Ensure 'text' column exists\n",
        "\n",
        "# Precompute TF-IDF for all texts\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text'].fillna(\"\"))\n",
        "idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
        "\n",
        "# Main extractor\n",
        "def extract_emnlp_complexity(text):\n",
        "    doc = nlp(str(text))\n",
        "    words = [token for token in doc if token.is_alpha]\n",
        "    tokens = [token.text.lower() for token in words]\n",
        "    word_count = len(words)\n",
        "\n",
        "    # === Lexical Complexity ===\n",
        "    avg_word_len = np.mean([len(w) for w in tokens]) if tokens else 0\n",
        "    poly_count = sum(1 for w in tokens if syllapy.count(w) >= 3)\n",
        "    poly_ratio = poly_count / word_count if word_count else 0\n",
        "\n",
        "    ttr = len(set(tokens)) / word_count if word_count else 0\n",
        "    connectives = {\"because\", \"since\", \"so\", \"if\", \"then\", \"but\", \"although\", \"and\", \"however\"}\n",
        "    avg_connectives = sum(1 for w in tokens if w in connectives) / word_count if word_count else 0\n",
        "\n",
        "    unique_entities = len(set(ent.text.lower() for ent in doc.ents))  # Named entities\n",
        "\n",
        "    # DALE Frequency\n",
        "    dale_score = dale_chall_readability_score(text)\n",
        "\n",
        "    # TF-IDF sum\n",
        "    tfidf_score = sum(idf_scores.get(w, 0) for w in tokens) / word_count if word_count else 0\n",
        "\n",
        "    # Log-likelihood ratio (placeholder with frequency)\n",
        "    word_freq = Counter(tokens)\n",
        "    lls = 0\n",
        "    for w, f in word_freq.items():\n",
        "        p = f / word_count\n",
        "        lls += f * log2(p) if p > 0 else 0\n",
        "\n",
        "    # === Syntactic Complexity ===\n",
        "    dep_lengths = []\n",
        "    idt = 0\n",
        "    le = 0\n",
        "    nested_noun_dists = []\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        heads = [token.head.i for token in sent if token.dep_ != \"ROOT\"]\n",
        "        deps = [token.i for token in sent if token.dep_ != \"ROOT\"]\n",
        "\n",
        "        dep_lengths.extend([abs(d - h) for d, h in zip(deps, heads)])\n",
        "\n",
        "        if not heads or len(heads) != len(deps):\n",
        "            idt += 1  # Incomplete dependency tree\n",
        "\n",
        "        # Left-embeddedness\n",
        "        for token in sent:\n",
        "            if token.head.i < token.i:\n",
        "                le += 1\n",
        "\n",
        "        # Nested noun phrases\n",
        "        np_indices = [token.i for token in sent if token.pos_ == \"NOUN\"]\n",
        "        if len(np_indices) > 1:\n",
        "            nested_noun_dists.append(np.std(np.diff(np_indices)))\n",
        "\n",
        "    return {\n",
        "        \"avg_word_length\": avg_word_len,\n",
        "        \"polysyllable_ratio\": poly_ratio,\n",
        "        \"dale_score\": dale_score,\n",
        "        \"type_token_ratio\": ttr,\n",
        "        \"connective_ratio\": avg_connectives,\n",
        "        \"unique_entities\": unique_entities,\n",
        "        \"avg_tfidf_score\": tfidf_score,\n",
        "        \"log_likelihood_score\": lls,\n",
        "        \"IDT_incomplete_deps\": idt,\n",
        "        \"DLT_avg_dependency_distance\": np.mean(dep_lengths) if dep_lengths else 0,\n",
        "        \"LE_left_embedding\": le,\n",
        "        \"NND_nested_noun_std\": np.mean(nested_noun_dists) if nested_noun_dists else 0\n",
        "    }\n",
        "\n",
        "# ðŸ” Apply\n",
        "feature_rows = df[\"text\"].apply(extract_emnlp_complexity)\n",
        "feature_df = pd.DataFrame(feature_rows.tolist())\n",
        "\n",
        "# âž• Save\n",
        "final_df = pd.concat([df, feature_df], axis=1)\n",
        "final_df.to_csv(\"Full_emnlp_complexity_features.csv\", index=False)\n",
        "\n",
        "print(\"EMNLP-style complexity features saved to 'sm_data_emnlp_complexity_features.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIEWo-yvvBjw",
        "outputId": "1fc04763-ef2b-4bd8-e6bd-d0e56016dbb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… EMNLP-style complexity features saved to 'sm_data_emnlp_complexity_features.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q spacy fasttext\n",
        "#!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N59NnvAUyB_D",
        "outputId": "56d89633-d5b5-4c9e-ebb5-9692ba382411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knfym4FkziuH",
        "outputId": "46a46723-b731-47fb-d377-8a7ea7bcb106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.11/dist-packages (0.9.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q fasttext\n",
        "!pip install -q spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JB3Zn5euz9Nk",
        "outputId": "e5b72e4c-b1a3-4167-b97a-7de7a87a77b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SoBQWQvW3fcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â¬‡ï¸ MUST RUN FIRST: Fix fasttext + numpy compatibility\n",
        "!pip install -q numpy==1.24.4 fasttext spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgxwOT9W3ff0",
        "outputId": "7d07c64a-1628-4bcd-b4db-50143878f4d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.5.1 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "xarray-einstats 0.9.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m120.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import fasttext\n",
        "import urllib.request\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load your tweet dataset (must contain a column named 'text')\n",
        "df = pd.read_csv(\"sm_data.csv\")\n",
        "df.columns = df.columns.str.strip().str.lower()  # Normalize column names\n",
        "assert 'text' in df.columns, \"The dataset must contain a 'text' column.\"\n",
        "\n",
        "# Download fastText language identification model\n",
        "ft_model_path = \"lid.176.bin\"\n",
        "if not os.path.exists(ft_model_path):\n",
        "    urllib.request.urlretrieve(\"https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\", ft_model_path)\n",
        "ft_model = fasttext.load_model(ft_model_path)\n",
        "\n",
        "# Detect language for each token using fastText\n",
        "def detect_langs(tokens):\n",
        "    langs = []\n",
        "    for token in tokens:\n",
        "        if token.strip():\n",
        "            label, _ = ft_model.predict(token)\n",
        "            lang = label[0].replace(\"__label__\", \"\")\n",
        "            langs.append(lang)\n",
        "    return langs\n",
        "\n",
        "# Compute Code-Mixing Metrics\n",
        "def code_mixing_metrics(text):\n",
        "    doc = nlp(str(text))\n",
        "    tokens = [token.text for token in doc if token.is_alpha]\n",
        "    if not tokens:\n",
        "        return dict.fromkeys(['CMI', 'M_index', 'I_index', 'Burstiness', 'Memory'], 0.0)\n",
        "\n",
        "    langs = detect_langs(tokens)\n",
        "    lang_counts = Counter(langs)\n",
        "    total = sum(lang_counts.values())\n",
        "\n",
        "    max_lang = max(lang_counts.values())\n",
        "    cmi = (total - max_lang) / total if total else 0\n",
        "\n",
        "    probs = [c / total for c in lang_counts.values()]\n",
        "    m_index = 1 - sum(p ** 2 for p in probs) if total > 1 else 0\n",
        "\n",
        "    switches = sum(1 for i in range(1, len(langs)) if langs[i] != langs[i - 1])\n",
        "    i_index = switches / (len(langs) - 1) if len(langs) > 1 else 0\n",
        "\n",
        "    bursts = []\n",
        "    prev = langs[0]\n",
        "    count = 1\n",
        "    for lang in langs[1:]:\n",
        "        if lang == prev:\n",
        "            count += 1\n",
        "        else:\n",
        "            bursts.append(count)\n",
        "            count = 1\n",
        "            prev = lang\n",
        "    bursts.append(count)\n",
        "    burstiness = np.std(bursts) / np.mean(bursts) if len(bursts) > 1 and np.mean(bursts) > 0 else 0\n",
        "\n",
        "    memory = sum(1 for i in range(1, len(langs)) if langs[i] == langs[i - 1]) / (len(langs) - 1) if len(langs) > 1 else 0\n",
        "\n",
        "    return {\n",
        "        \"CMI\": round(cmi, 4),\n",
        "        \"M_index\": round(m_index, 4),\n",
        "        \"I_index\": round(i_index, 4),\n",
        "        \"Burstiness\": round(burstiness, 4),\n",
        "        \"Memory\": round(memory, 4)\n",
        "    }\n",
        "\n",
        "# Apply to all texts\n",
        "code_mixed = df[\"text\"].apply(code_mixing_metrics)\n",
        "metrics_df = pd.DataFrame(code_mixed.tolist())\n",
        "\n",
        "# Save combined result\n",
        "result = pd.concat([df, metrics_df], axis=1)\n",
        "result.to_csv(\"Full_code_mixed_complexity.csv\", index=False)\n",
        "\n",
        "# Done\n",
        "print(\"Code-mixed complexity metrics saved to 'sm_data_code_mixed_complexity.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cMhH5M73jcD",
        "outputId": "ed08d69f-9b85-4a3e-e2e9-e140308d87ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Code-mixed complexity metrics saved to 'sm_data_code_mixed_complexity.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uMTzUIVD3jfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy"
      ],
      "metadata": {
        "id": "ql6kmkN06ByV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Install required packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import gzip\n",
        "import io\n",
        "from collections import Counter\n",
        "\n",
        "# Load SpaCy model\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "#Load your social media dataset (ensure it has a 'text' column)\n",
        "df = pd.read_csv(\"sm_data.csv\")\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "assert 'text' in df.columns, \"Dataset must have a 'text' column.\"\n",
        "\n",
        "# Yule's K-complexity function\n",
        "def yules_k(text):\n",
        "    doc = nlp(str(text).lower())\n",
        "    words = [token.text for token in doc if token.is_alpha]\n",
        "    if not words:\n",
        "        return 0\n",
        "    freq = Counter(words)\n",
        "    N = sum(freq.values())\n",
        "    freqs_of_freqs = Counter(freq.values())\n",
        "    M = sum(f * f_count for f, f_count in freqs_of_freqs.items())\n",
        "    K = (10_000 * (M - N)) / (N * N) if N > 0 else 0\n",
        "    return round(K, 4)\n",
        "\n",
        "# Gzip compression complexity\n",
        "def gzip_ratio(text):\n",
        "    raw = text.encode('utf-8')\n",
        "    if not raw:\n",
        "        return 0.0\n",
        "    with io.BytesIO() as bio:\n",
        "        with gzip.GzipFile(fileobj=bio, mode='w') as f:\n",
        "            f.write(raw)\n",
        "        compressed = bio.getvalue()\n",
        "    ratio = len(compressed) / len(raw) if len(raw) > 0 else 0\n",
        "    return round(ratio, 4)\n",
        "\n",
        "# Apply to dataset\n",
        "df[\"yules_k\"] = df[\"text\"].apply(yules_k)\n",
        "df[\"gzip_complexity\"] = df[\"text\"].apply(gzip_ratio)\n",
        "\n",
        "# Save output\n",
        "df.to_csv(\"Full_comment_complexity.csv\", index=False)\n",
        "print(\"Complexity metrics (Yule's K and gzip) saved to 'sm_data_comment_complexity.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d3KEa046B1W",
        "outputId": "2b86958e-62cf-4b0a-8381-147ba138fbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Complexity metrics (Yule's K and gzip) saved to 'sm_data_comment_complexity.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zCEAQ-nHDSCP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}